{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bbf6ec6-08f3-45ed-913e-1a6e86284920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying tweets by the stock they are talking about before we drop the tweets in the conversation.\n",
    "# Ctrl+f \"Wyatt\" to see my comments on the code.\n",
    "import warnings\n",
    "from time import time\n",
    "import re\n",
    "from typing import List\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import explode, udf, size, col, concat_ws, to_date, to_timestamp, date_trunc, count, array_contains, month, dayofmonth, hour, explode_outer\n",
    "from pyspark.sql.types import StringType, ArrayType, Row\n",
    "\n",
    "stocks = [\"AAPL\", \"MSFT\", \"AMZN\", \"TSLA\", \"GOOG\", \"GOOGL\", \"META\", \"NVDA\", \"PEP\", \"COST\", \"AVGO\", \"CSCO\", \"TMUS\",\n",
    "          \"ADBE\", \"TXN\", \"CMCSA\", \"AMGN\", \"QCOM\", \"NFLX\", \"HON\", \"INTU\", \"INTC\", \"SBUX\", \"PYPL\", \"ADP\", \"AMD\",\n",
    "          \"GILD\", \"MDLZ\", \"REGN\", \"ISRG\", \"VRTX\", \"ADI\", \"BKNG\", \"AMAT\", \"FISV\", \"CSX\", \"MU\", \"ATVI\", \"KDP\", \"CHTR\",\n",
    "          \"MAR\", \"MRNA\", \"PANW\", \"ORLY\", \"ABNB\", \"MNST\", \"LRCX\", \"KHC\", \"SNPS\", \"AEP\", \"ADSK\", \"CDNS\", \"MELI\",\n",
    "          \"CTAS\", \"FTNT\", \"PAYX\", \"KLAC\", \"BIIB\", \"DXCM\", \"NXPI\", \"EXC\", \"ASML\", \"LULU\", \"EA\", \"XEL\", \"MCHP\",\n",
    "          \"CRWD\", \"MRVL\", \"AZN\", \"ILMN\", \"PCAR\", \"DLTR\", \"CTSH\", \"WDAY\", \"ROST\", \"ODFL\", \"WBA\", \"CEG\", \"IDXX\",\n",
    "          \"TEAM\", \"VRSK\", \"FAST\", \"CPRT\", \"PDD\", \"SGEN\", \"SIRI\", \"DDOG\", \"LCID\", \"ZS\", \"JD\", \"EBAY\", \"VRSN\", \"ZM\",\n",
    "          \"ANSS\", \"BIDU\", \"ALGN\", \"SWKS\", \"MTCH\", \"SPLK\", \"NTES\", \"DOCU\", \"OKTA\"]\n",
    "\n",
    "\n",
    "def make_regex():\n",
    "    regex = \"((\\$|\\#|\\＃)(\"\n",
    "    for stock in stocks:\n",
    "        regex = regex + stock + \"|\"\n",
    "    regex = regex[:-1]\n",
    "    regex = regex + \")(?![a-z|A-Z]))\"\n",
    "    return regex\n",
    "\n",
    "\n",
    "ticker_matcher = make_regex()\n",
    "# print(ticker_matcher)\n",
    "ticker_matcher = re.compile(ticker_matcher, re.IGNORECASE)\n",
    "\n",
    "\n",
    "def write_db(save_df: DataFrame, table_name: str, mode=\"overwrite\"):\n",
    "    save_df.write.format(\"jdbc\") \\\n",
    "        .mode(mode) \\\n",
    "        .option(\"url\", \"jdbc:mysql://127.0.0.1:3306/cs179g\") \\\n",
    "        .option(\"driver\", \"com.mysql.cj.jdbc.Driver\") \\\n",
    "        .option(\"dbtable\", f\"{table_name}\") \\\n",
    "        .option(\"user\", \"group6\") \\\n",
    "        .option(\"batchsize\", \"100000\") \\\n",
    "        .option(\"password\", \"grp6\").save()\n",
    "\n",
    "\n",
    "def extract_cash_tags(col_value: str) -> list[str]:\n",
    "    return list(map(lambda x: str(x).upper(), re.findall(r\"$(\\w+)\", col_value))) if col_value else []\n",
    "\n",
    "\n",
    "def extract_hash_tags(col_value: str) -> list[str]:\n",
    "    return list(map(lambda x: str(x).upper(), re.findall(r\"#(\\w+)\", col_value))) if col_value else []\n",
    "\n",
    "\n",
    "def tweets_to_tickers(tweets, baseTweetText) -> list[str]:\n",
    "    if tweets is None:\n",
    "        return None\n",
    "    if baseTweetText is None:\n",
    "        return None\n",
    "    tweets_text = baseTweetText\n",
    "    # print(f\"\\nBase tweet's text:{tweets_text}\")\n",
    "    for item in tweets:\n",
    "        tweets_text = tweets_text + \"     \" + item.text\n",
    "    tweets_text = tweets_text + \" \"\n",
    "    # print(f\"\\nConversation's text:{tweets_text}\")\n",
    "\n",
    "    stockList = ticker_matcher.findall(tweets_text)\n",
    "    _stockList = []\n",
    "    stockListEmpty = 1\n",
    "    for item in stockList:\n",
    "        _stockList.append(item[2].upper())\n",
    "        stockListEmpty = 0\n",
    "    stockList = [*set(_stockList)]\n",
    "    stockList.sort()\n",
    "    if stockListEmpty:\n",
    "        print(f\"\\n\\nStockList empty!:\\n\\n{tweets_text}\\n\\n\")\n",
    "    # print(stockList)\n",
    "    return stockList\n",
    "\n",
    "\n",
    "tweets_to_tickers_UDF = udf(\n",
    "    lambda x, y: tweets_to_tickers(x, y), ArrayType(StringType()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ac3d37b-57bf-4107-a013-97deac81c353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/ubuntu/your_venv/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/ubuntu/.ivy2/cache\n",
      "The jars for the packages stored in: /home/ubuntu/.ivy2/jars\n",
      "mysql#mysql-connector-java added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-8d4ea119-2d60-4df1-9564-899f0f2ca60b;1.0\n",
      "\tconfs: [default]\n",
      "\tfound mysql#mysql-connector-java;8.0.30 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.19.4 in central\n",
      ":: resolution report :: resolve 136ms :: artifacts dl 6ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.protobuf#protobuf-java;3.19.4 from central in [default]\n",
      "\tmysql#mysql-connector-java;8.0.30 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-8d4ea119-2d60-4df1-9564-899f0f2ca60b\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/6ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/19 21:38:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/19 21:38:23 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/11/19 21:38:23 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "22/11/19 21:38:23 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('practice').config('spark.jars.packages', 'mysql:mysql-connector-java:8.0.30').master(\n",
    "    \"local\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f3ba149-93b0-4f37-9a10-ea9e6402906c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/19 21:38:27 WARN DataSource: All paths were ignored:\n",
      "  file:/home/ubuntu/Part2_test/h_o/.ipynb_checkpoints\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/19 21:38:32 WARN DataSource: All paths were ignored:\n",
      "  file:/home/ubuntu/Part2_test/c_o/.ipynb_checkpoints\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Count1-3 22935, 22935, 22935\n",
      "Number of EBAY tweets with hashtags: 9649\n",
      "Number of EBAY tweets with cashtags: 178\n",
      "Filter 1: 9085\n",
      "Filter 2: 4850\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----+-------+-----------+\n",
      "|month|day|hour|tickers|       tags|\n",
      "+-----+---+----+-------+-----------+\n",
      "|   10| 26|   8|     EA|APEXLEGENDS|\n",
      "|   10| 26|   8|     EA|    RESPAWN|\n",
      "|   10| 26|   8|     EA|         EA|\n",
      "|   10| 26|   8|     EA|       APEX|\n",
      "|   10| 26|   8|     EA|    SAVIORS|\n",
      "+-----+---+----+-------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# change directories back -------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#print(\"Reading data\")\n",
    "hashtag_df: DataFrame = spark.read.option(\"multiLine\", \"true\").option(\"mode\", \"PERMISSIVE\").json(\n",
    "    \"h_o/.*\")\n",
    "\n",
    "cashtag_df: DataFrame = spark.read.option(\"multiLine\", \"true\").option(\"mode\", \"PERMISSIVE\").json(\n",
    "    \"c_o/.*\")\n",
    "\n",
    "# print(\"Done\")\n",
    "#hashtag_df = hashtag_df.union(cashtag_df)\n",
    "#print(\"Exploding tweets\")\n",
    "# Wyatt: We need to select col.includes.tweets as well to get the context of a tweet.\n",
    "tweets_hashtag_data_df = hashtag_df.select(\n",
    "    explode(\"tweets\")).select(\"col.data.*\", \"col.includes.tweets\")\n",
    "tweets_cashtag_data_df = cashtag_df.select(\n",
    "    explode(\"tweets\")).select(\"col.data.*\", \"col.includes.tweets\")\n",
    "# print(\"Done\")\n",
    "count1 = tweets_hashtag_data_df.count()\n",
    "\n",
    "tweets_hashtag_identified_df = tweets_hashtag_data_df.withColumn(\n",
    "    \"tickers\", tweets_to_tickers_UDF(col(\"tweets\"), col(\"text\")))\n",
    "tweets_cashtag_identified_df = tweets_cashtag_data_df.withColumn(\n",
    "    \"tickers\", tweets_to_tickers_UDF(col(\"tweets\"), col(\"text\")))\n",
    "\n",
    "count2 = tweets_hashtag_identified_df.count()\n",
    "\n",
    "#print(\"Extracting cash and hashtags\")\n",
    "hashtags_df = tweets_hashtag_identified_df.select(\"author_id\", \"created_at\", \"public_metrics.*\", \"text\", \"tickers\").withColumn(\n",
    "    \"tags\", udf(extract_hash_tags, ArrayType(StringType()))(\"text\"))\n",
    "\n",
    "cashtags_df = tweets_cashtag_identified_df.select(\"author_id\", \"created_at\", \"public_metrics.*\", \"text\", \"tickers\").withColumn(\n",
    "    \"tags\", udf(extract_cash_tags, ArrayType(StringType()))(\"text\"))\n",
    "\n",
    "count3 = hashtags_df.count()\n",
    "\n",
    "# Filtering out results from seperate cashtag and hashtag dataframes happens here\n",
    "ebay_hashtags_df = hashtags_df.where(\n",
    "    array_contains(col(\"tickers\"), \"EBAY\"))\n",
    "ebay_cashtags_df = cashtags_df.where(\n",
    "    array_contains(col(\"tickers\"), \"EBAY\"))\n",
    "\n",
    "ebay_h_count = ebay_hashtags_df.count()\n",
    "ebay_c_count = ebay_cashtags_df.count()\n",
    "ebay_hashtags_df_f1 = ebay_hashtags_df.where(\n",
    "    ~array_contains(col(\"tags\"), \"BOUTIQUE\") & ~array_contains(col(\"tags\"), \"SEARCHNCOLLECT\") & ~array_contains(col(\"tags\"), \"ALIEXPRESS\"))\n",
    "ebay_hashtags_df_f1_count = ebay_hashtags_df_f1.count()\n",
    "ebay_hashtags_df_f2 = ebay_hashtags_df_f1.where(\n",
    "    ~ebay_hashtags_df_f1.text.like(\"Check out%\")\n",
    ")\n",
    "\n",
    "\n",
    "print(\n",
    "    f\"\\nCount1-3 {count1}, {count2}, {count3}\\nNumber of EBAY tweets with hashtags: {ebay_h_count}\\nNumber of EBAY tweets with cashtags: {ebay_c_count}\\nFilter 1: {ebay_hashtags_df_f1_count}\\nFilter 2: {ebay_hashtags_df_f2.count()}\\n\")\n",
    "\n",
    "# print(\"Done\")\n",
    "# print(\"Merging\")\n",
    "#  merge two dataframe\n",
    "union_df = hashtags_df.union(cashtags_df).withColumn(\"created_at\",\n",
    "                                                     to_timestamp(\"created_at\", \"yyyy-MM-dd'T'HH:mm:ss.SSSX\"))\n",
    "# print(\"Done\")\n",
    "# print(\"Tags_df\")\n",
    "# tags_df = union_df.filter(size(\"tags\") > 0).withColumn(\n",
    "#     \"tag\", explode(\"tags\"))\n",
    "# # 1 top 5 stock with count #$ only\n",
    "# tags_df.drop(\"tags\").createOrReplaceTempView(\"tag\")\n",
    "\n",
    "# Now get popularity statistics of stocks\n",
    "union_df = union_df.withColumn(\"month\", month(\"created_at\")).withColumn(\n",
    "    \"day\", dayofmonth(\"created_at\")).withColumn(\"hour\", hour(\"created_at\"))\n",
    "union_df = union_df.select(\"month\", \"day\", \"hour\", \"like_count\", \"quote_count\",\n",
    "                           \"reply_count\", \"retweet_count\", \"text\", \"tickers\", \"tags\")\n",
    "\n",
    "\n",
    "time_tickers_tags_df = union_df.select(\n",
    "    \"month\", \"day\", \"hour\", \"tickers\", \"tags\")\n",
    "\n",
    "\n",
    "# time_tickers_tags_explode_df = time_tickers_tags_df.select()\n",
    "\n",
    "union_df.createOrReplaceTempView(\"union\")\n",
    "ticker_freq_df: DataFrame = spark.sql(\n",
    "    \"select tickers, count(*) as cnt from union group by tickers order by cnt desc\")\n",
    "\n",
    "all_tickers_df = union_df.select(\n",
    "    explode_outer(\"tickers\").alias(\"exploded\"))\n",
    "\n",
    "\n",
    "time_tickers_explode_df = time_tickers_tags_df.withColumn(\n",
    "    \"tickers\", explode_outer(\"tickers\"))\n",
    "\n",
    "time_tickers_tags_explode_df = time_tickers_explode_df.withColumn(\n",
    "    \"tags\", explode_outer(\"tags\"))\n",
    "time_tickers_tags_explode_df.show(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0215f544-0d67-48f9-9feb-ac4709b79529",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_tickers_tags_explode_df.createOrReplaceTempView(\"time_tickers_tags_explode_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "40184d07-25de-454c-89a7-b228cc2c6cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "z=spark.sql(\"Select month, day, hour, tickers, tags, count(tags) as cnt_t from time_tickers_tags_explode_df group by month, day, hour, tickers, tags\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2a7f2dde-bb02-4b2c-8101-b3ac17608f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "z.createOrReplaceTempView(\"z\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b6ab3924-b8c4-4881-a1d0-1d2c187b8411",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=spark.sql(\"select month,day,hour,tickers,tags, sum(cnt_t) from z group by month, day, hour, tickers,tags order by 5 desc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "459ffa05-5033-4c56-aab9-c9f013510049",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 73:==========================================================(2 + 0) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----+-------+------------------------+----------+\n",
      "|month|day|hour|tickers|                    tags|sum(cnt_t)|\n",
      "+-----+---+----+-------+------------------------+----------+\n",
      "|   10| 26|  16|   EBAY|ＨＵＮＴＥＲＨＵＮＴＥＲ|         1|\n",
      "|   10| 26|   9|     MU|                퍼피러브|         1|\n",
      "|   10| 26|  15|     MU|                퍼피러브|         1|\n",
      "|   10| 26|  15|    AMD|            최고의플레이|         1|\n",
      "|   10| 26|  13|   TSLA|  지구에서년지성아환영해|         1|\n",
      "|   10| 26|   9|     ZS|                  조로산|         1|\n",
      "|   10| 26|   8|     MU|                    제프|         1|\n",
      "|   10| 26|  11|     MU|         제니스_어화둥둥|         2|\n",
      "|   10| 26|  13|     MU|         제니스_어화둥둥|         1|\n",
      "|   10| 26|  15|     MU|                  제니스|         9|\n",
      "|   10| 26|  15|     MU|                  이펙스|        56|\n",
      "|   10| 26|  10|     MU|                  이펙스|         7|\n",
      "|   10| 26|  12|     MU|                  이펙스|         2|\n",
      "|   10| 26|  16|     MU|                  이펙스|        22|\n",
      "|   10| 26|  11|     MU|                  이펙스|         5|\n",
      "|   10| 26|   8|     MU|                  이펙스|         2|\n",
      "|   10| 26|   9|     MU|                  이펙스|         2|\n",
      "|   10| 26|  13|     MU|                  이펙스|        47|\n",
      "|   10| 26|  14|     MU|                  이펙스|        89|\n",
      "|   10| 26|  16|     MU|                    위시|         1|\n",
      "+-----+---+----+-------+------------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "x.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "dc726dc3-2705-4fbd-af81-b8351d09fc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "z2=spark.sql(\"Select month, day, tickers, tags, count(tags) as cnt_t from time_tickers_tags_explode_df group by month, day,tickers, tags\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7ca58045-eb21-4668-acd2-dc3e2aa472bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "z2.createOrReplaceTempView(\"z2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b7239b42-472d-4228-8cfe-e4f016bf7984",
   "metadata": {},
   "outputs": [],
   "source": [
    "x2=spark.sql(\"select month,day, tickers,tags, sum(cnt_t) from z2 group by month, day,tickers,tags order by 5 desc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e63988f4-92c3-4e1e-930e-08e35669a720",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 76:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-------+------------+----------+\n",
      "|month|day|tickers|        tags|sum(cnt_t)|\n",
      "+-----+---+-------+------------+----------+\n",
      "|   10| 26|   EBAY|        EBAY|      9293|\n",
      "|   10| 26|   TEAM|        TEAM|      1847|\n",
      "|   10| 26|   META|        META|      1254|\n",
      "|   10| 26|     MU|          MU|      1190|\n",
      "|   10| 26|   TEAM|        DEFI|      1079|\n",
      "|   10| 26|   TEAM|       GRAPE|       941|\n",
      "|   10| 26|   TEAM|       SAFUU|       941|\n",
      "|   10| 26|   TEAM|  STABLEFUND|       941|\n",
      "|   10| 26|   TEAM|        DRIP|       941|\n",
      "|   10| 26|   TEAM|         EMP|       941|\n",
      "|   10| 26|   TEAM|     CAPTAIN|       939|\n",
      "|   10| 26|     MU|MISSUNIVERSE|       909|\n",
      "|   10| 26|     EA|          FX|       903|\n",
      "|   10| 26|     MU|       JKN18|       884|\n",
      "|   10| 26|     MU|         MUO|       879|\n",
      "|   10| 26|   TEAM|BATTLESTAKES|       752|\n",
      "|   10| 26|   TEAM|  JOINTAKEDA|       752|\n",
      "|   10| 26|     EA|          EA|       658|\n",
      "|   10| 26|     MU|        แอนจ|       623|\n",
      "|   10| 26|    WBA|         WBA|       576|\n",
      "+-----+---+-------+------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "x2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a29a4c26-6c59-42d6-a7b9-786fe04b4026",
   "metadata": {},
   "outputs": [],
   "source": [
    "z3=spark.sql(\"Select month, tickers, tags, count(tags) as cnt_t from time_tickers_tags_explode_df group by month,tickers, tags\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0deb86ff-a542-4660-b0de-63893e9d01f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "z3.createOrReplaceTempView(\"z3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "40cd7480-391d-4389-b4b1-63cc7068fa00",
   "metadata": {},
   "outputs": [],
   "source": [
    "x3=spark.sql(\"select month, tickers,tags, sum(cnt_t) from z3 group by month, tickers,tags order by 3 desc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "afdc6c63-4630-42de-b4c8-8441f3e82250",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 79:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+------------------------+----------+\n",
      "|month|tickers|                    tags|sum(cnt_t)|\n",
      "+-----+-------+------------------------+----------+\n",
      "|   10|   EBAY|ＨＵＮＴＥＲＨＵＮＴＥＲ|         1|\n",
      "|   10|     MU|                퍼피러브|         2|\n",
      "|   10|    AMD|            최고의플레이|         1|\n",
      "|   10|   TSLA|  지구에서년지성아환영해|         1|\n",
      "|   10|     ZS|                  조로산|         1|\n",
      "|   10|     MU|                    제프|         1|\n",
      "|   10|     MU|         제니스_어화둥둥|         3|\n",
      "|   10|     MU|                  제니스|         9|\n",
      "|   10|     MU|                  이펙스|       232|\n",
      "|   10|     MU|                    위시|         8|\n",
      "|   10|     MU|                    예왕|         1|\n",
      "|   10|   TSLA|                에이티즈|         1|\n",
      "|   10|     MU|                  에이든|         1|\n",
      "|   10|     MU|              업고서놀자|         3|\n",
      "|   10|     MU|                    아민|         2|\n",
      "|   10|     MU|                  서경민|       153|\n",
      "|   10|     MU|                사랑의서|         2|\n",
      "|   10|     MU|                  사랑가|         4|\n",
      "|   10|     MU|                  사랑歌|         5|\n",
      "|   10|     MU|                    백승|         1|\n",
      "+-----+-------+------------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "x3.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
